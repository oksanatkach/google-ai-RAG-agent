{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3fd2b3d-ff4a-478e-add3-46e297b9cc80",
   "metadata": {},
   "source": [
    "# ðŸ§© ACME Q&A Agent Assignment\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: Agent and Tooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf41ce3e-91ee-4af6-9a41-1188df48e452",
   "metadata": {},
   "source": [
    "1. **Google AI Studio Application**\n",
    "    Google AI Studio app: https://ai.studio/apps/drive/1FYrTQywe4P9926W_LH4epOd-e6sjdmk9\n",
    "\n",
    "2. **Exported Conversation JSON Files**\n",
    "    `data/part_1_scenarios`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a71a6e1-7872-4387-8389-45f79bdc924a",
   "metadata": {},
   "source": [
    "The agent was generated using Google AI Studio's Gemini, however the resulting code base did not connect smoothly and required some debugging. The GeminiAgent was receiving incorrect outputs from ACME API, and I had to fix this manually by properly formatting tool output.\n",
    "\n",
    "Most of the scenarios requested in task part 1 are already fulfilled by @google/genai implementation. For instance, the reasoning chain and multi-hop reasoning it produces likely come from how `gemini-2.5-flash` was trained.\n",
    "\n",
    "I had the most trouble with scenario `Scenario 4: Ambiguous Query Requiring Disambiguation`. This is likely because in the @google/genai implementation, the agent is encouraged to call the tools. When I added the instruction to ask clarifying questions before calling tools as simply one of the rules in the system prompt, it didn't work and was overriden by agent implementation every time. I had to re-design the entire system prompt to force the agent to first \"classify\" user query and then decide when and how to call the tools like the following:\n",
    "\n",
    "\n",
    ">STEP 1: ASSESS QUERY CLARITY\n",
    ">Ask yourself: \"Is this query specific enough to answer?\"\n",
    ">\n",
    ">VAGUE QUERIES - Ask clarifying questions WITHOUT calling tools:\n",
    ">Examples:\n",
    ">- \"Tell me about ACME's people\" â†’ Ask: \"Would you like to know about our leadership team, HR policies, employee benefits, or something >else?\"\n",
    ">- \"What products do we have?\" â†’ Ask: \"Are you interested in a complete list, specific product categories, or details about a particular >product?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b7bda5-29a9-44e6-90b4-ae2c3e7a9120",
   "metadata": {},
   "source": [
    "## Part 2: Evaluation and Learning Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d6c20e-d290-410b-a707-535de935ae1d",
   "metadata": {},
   "source": [
    "### Insights From Logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1b9739-a45b-4d5d-9ca1-de90838aff09",
   "metadata": {},
   "source": [
    "In a way, this agent is too good, because I've used up all free requests on three separate Google accounts to find any issues.\n",
    "\n",
    "The QA works well on concrete, factoid questions.\n",
    "\n",
    "The issues are revealed when the conversations get longer. This requires having multiple conversations with over 5 turns, which quickly depletes request quotas.\n",
    "\n",
    "Due to time constraints and request quotas, I have decided to stop at two concrete issues I found:\n",
    "\n",
    "- *redundant calls*: the agent very often does not consult conversation history and keeps sending requests to the same file;\n",
    "- *insufficient reasoning search / gives up fast*: the agent does not consult conversation history nor queries for another document, but states the answer to the question is not in the file or tries to ask clarifying questions instead of looking for relevant information in the document.\n",
    "\n",
    "I have implement two functions to deterministically check if the above issues appear in conversations. I also have a quick citation check function to demonstrate a check that works relatively well and can be saved to insights. I made sure to make these metrics applicable to all conversations with concrete outputs. This allows the insights to be used in regression tests: the metric values should be checked every time the agent is updated to check if fixing one issue did not break any other components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ce473f-04d1-4003-8704-bc4eea5cdc8d",
   "metadata": {},
   "source": [
    "#### No Citations\n",
    "\n",
    "Additional insight: is there a citation and is it valid; this one rarely fails.\n",
    "\n",
    "This function will return False for cases where the question does not appear in the documents, in which case it is correct to not have a citation. It is normal to always have some percentage of responces uncited, but it's still a good statistic to trace with agent updates.\n",
    "\n",
    "This functon can be improved by further disambiguating whether the citation was not in the answer at all vs wrong file was cited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c45e3f31-d828-4a0e-96ee-b0d8d3cb40a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cite_prefixes = ['source:', 'from:', 'based on:', 'sources:']\n",
    "prefix_regex = r'%s (.*\\.mdc)'\n",
    "\n",
    "def correct_sources_cited(turn):\n",
    "    doc_ids_from_tools = set([call['arguments']['id'] if 'id' in call['arguments'] else None for call in turn['tool_invocations']])\n",
    "    if not doc_ids_from_tools:\n",
    "        return False\n",
    "    \n",
    "    model_response = turn['model_response']['content'].replace('\\n', ' ').lower()\n",
    "    \n",
    "    for cite_prefix in cite_prefixes:\n",
    "        cited_docs = re.search(prefix_regex % cite_prefix, model_response)\n",
    "        if cited_docs:\n",
    "            cited_docs = set(cited_docs.group(1).split(', '))\n",
    "            if len(cited_docs.intersection(doc_ids_from_tools)):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def count_no_citations(conversation_json):\n",
    "    not_cited = 0\n",
    "    for turn in conversation_json['turns']:\n",
    "        if turn['type'] != 'user_query':\n",
    "            continue\n",
    "        \n",
    "        if not correct_sources_cited(turn):\n",
    "            not_cited += 1\n",
    "    return not_cited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4ef29c-633c-4cb1-b027-5cce087c25eb",
   "metadata": {},
   "source": [
    "#### Redundant calls\n",
    "\n",
    "We can easily find if there are redundant calls by checking if any document id was called more than once by the same tool within one conversation.\n",
    "\n",
    "The goal is to have 0 redundant calls on all conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "061538f3-b57b-4668-83bd-8192948c6048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d0602ad-ca04-4f6a-bd89-823f0d2aa1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_redundant_calls(conversation_json):\n",
    "    num_redundant_calls = 0\n",
    "    tool_invocations = [call for turn in conversation_json['turns'][1:] for call in turn['tool_invocations']]\n",
    "    \n",
    "    doc_id_outline = [call['arguments']['id'] for call in tool_invocations if call['tool_name'] == 'getOutline']\n",
    "    doc_id_full = [call['arguments']['id'] for call in tool_invocations if call['tool_name'] == 'getFull']\n",
    "\n",
    "    # difference between document queried and number of unique documents queried\n",
    "    num_redundant_calls += len(doc_id_outline) - len(set(doc_id_outline))\n",
    "    num_redundant_calls += len(doc_id_full) - len(set(doc_id_full))\n",
    "    \n",
    "    return num_redundant_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34397b2d-d5e8-4b43-b718-2b679f64dc5e",
   "metadata": {},
   "source": [
    "#### Insufficient Search\n",
    "\n",
    "There are many questions to which the agent is not able to answer, and a reply like \"I am unable to answer\" is appropriate. However, for queries to which we know the answer is in the document, we can detect early giving up by scanning for keywords. This insight will flag both unanswearable and answearable questions, but we can additionally label answearable questions and track this metric for them. We can also track this metric in general to see if the agent has more successful conversations with added changes to the code base / expanded document collection.\n",
    "\n",
    "**Note:** This is essentially a bad retrieval issue. This deterministic check can be made more accurate by using some NLP techniques. For instance, we can check if the main constituent phrases from the user query appear in any documents which the agent did not call. A more advanced version of this is BM25 search. A BM25 retriever is relatively light and we can compare LLM reasoning performance against retriever matches while regression tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0fe7548-aa51-45c6-ba03-8ebdd807346d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_insufficient_search(conversation_json):\n",
    "    \"\"\"\n",
    "    Detects when agent gives up without adequate searching\n",
    "    \"\"\"\n",
    "\n",
    "    for turn in conversation_json['turns']:\n",
    "        if turn['type'] != 'user_query':\n",
    "            continue\n",
    "        \n",
    "        response = turn['model_response']['content'].lower()\n",
    "        tool_calls = turn.get('tool_invocations', [])\n",
    "        \n",
    "        # Keywords indicating agent gave up\n",
    "        gave_up_phrases = [\n",
    "            \"don't have information\",\n",
    "            \"not available in the documentation\",\n",
    "            \"isn't in the documentation\",\n",
    "            \"couldn't find\",\n",
    "            \"not in the available documentation\",\n",
    "            \"can you clarify\",\n",
    "            \"which would you like to know\",\n",
    "            \"can you be more specific\",\n",
    "            \"I'm sorry\",\n",
    "            \"I cannot determine\",\n",
    "        ]\n",
    "        \n",
    "        agent_gave_up = any(phrase in response for phrase in gave_up_phrases)\n",
    "        \n",
    "        if agent_gave_up:\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9785cafc-ec1a-4f58-a72a-bdaec8a96c61",
   "metadata": {},
   "source": [
    "#### Extract Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71755773-9cbd-4383-8efe-6a3df9dac312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_insights(conversations_path):\n",
    "    insights = []\n",
    "    \n",
    "    for root, folder, files in os.walk(conversations_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                conv_jsn = json.load(open(os.path.join(root, file)))\n",
    "    \n",
    "                redundant_calls = count_redundant_calls(conv_jsn)\n",
    "                insufficient_search = detect_insufficient_search(conv_jsn)\n",
    "                no_citations = count_no_citations(conv_jsn)\n",
    "    \n",
    "                insights.append({\n",
    "                    \"conversation_id\": conv_jsn[\"conversation_id\"],\n",
    "                    \"redundant_calls\": redundant_calls,\n",
    "                    \"insufficient_search\": insufficient_search,\n",
    "                    \"no_citations\": no_citations\n",
    "                })\n",
    "    return insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad4fec5d-28fb-460b-9fba-26a98408f449",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations_path = 'data/part_2_conversations/run_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b043f2b-82b9-4b5c-bb90-364612a3c6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "insights = get_insights(conversations_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "165201a3-5b3e-49d1-aa5a-a1ad4613e37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_redundant_calls = sum([convo[\"redundant_calls\"] for convo in insights])\n",
    "num_insufficient_search = sum([convo[\"insufficient_search\"] for convo in insights])\n",
    "num_no_citations = sum([convo[\"no_citations\"] for convo in insights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be2c258a-230f-423e-8d2f-3c5f023a272c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redundant calls: 11\n",
      "Apparently insufficient search: 4\n",
      "Answers with no citations: 31\n"
     ]
    }
   ],
   "source": [
    "print(f\"Redundant calls: {num_redundant_calls}\")\n",
    "print(f\"Apparently insufficient search: {num_insufficient_search}\")\n",
    "print(f\"Answers with no citations: {num_no_citations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61f044dc-7c86-4e0f-85ed-841b9d4cafed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: change path\n",
    "json.dump(insights, open('data/insights_run1.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938031b3-d0de-45a3-8fbf-af7809982b5c",
   "metadata": {},
   "source": [
    "### Insight Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa83e9a-072b-48c6-8e56-f79ca19e7567",
   "metadata": {},
   "source": [
    "It's not really possible to automatically (deterministically) improve the system prompt from the insights, unless we want to prompt an LLM to generate a new system prompt for us.\n",
    "\n",
    "But we can run all the same questions with an updated system prompt and see if the number of redundant calls and apparently insufficient search have gone down.\n",
    "\n",
    "\n",
    "I believe we can fix both issues with a single system prompt patch. After many tries, I finally got the agent to reduce the number of calls by forcing it to first generate a list of documents retrieved within the conversation. Added to the system prompt:\n",
    "\n",
    "\n",
    "\n",
    ">REQUIRED TWO-PHASE RESPONSE PATTERN:\n",
    ">\n",
    ">PHASE 1 - INVENTORY CHECK (You must always do this first):\n",
    ">State explicitly:\n",
    ">\"Let me check what I already know:\n",
    ">- Documents retrieved so far: [list]\n",
    ">- Information I already have: [brief summary]\n",
    ">- Do I need new documents? [YES/NO]\"\n",
    ">\n",
    ">PHASE 2 - ACTION:\n",
    ">- If you said \"NO\" in Phase 1 â†’ Answer using existing information\n",
    ">- If you said \"YES\" in Phase 1 â†’ Call getFull ONLY on NEW documents\n",
    ">\n",
    ">You are FORBIDDEN from calling tools until you complete Phase 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11327701-9ecc-40e1-91db-e2e18978ceb2",
   "metadata": {},
   "source": [
    "Ideally, I should re-run all conversations from run 1, but I have run out of requests, so I only ran the problematic ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e50476d-4c6a-4ec1-9082-013a09527bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations_path_run2 = 'data/part_2_conversations/run_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c749481-3b2b-444f-8909-5c2f0bd5c2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "insights_run2 = get_insights(conversations_path_run2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd2bb748-6b87-4000-8a98-6dd00de3af29",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_redundant_calls_run2 = sum([convo[\"redundant_calls\"] for convo in insights_run2])\n",
    "num_insufficient_search_run2 = sum([convo[\"insufficient_search\"] for convo in insights_run2])\n",
    "num_no_citations_run2 = sum([convo[\"no_citations\"] for convo in insights_run2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5285189b-d149-469f-ba20-14e6e72add0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redundant calls: 1\n",
      "Apparently insufficient search: 0\n",
      "Answers with no citations: 12\n"
     ]
    }
   ],
   "source": [
    "print(f\"Redundant calls: {num_redundant_calls_run2}\")\n",
    "print(f\"Apparently insufficient search: {num_insufficient_search_run2}\")\n",
    "print(f\"Answers with no citations: {num_no_citations_run2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbb6c25-ca63-4ead-976b-a5819a94e94a",
   "metadata": {},
   "source": [
    "These issues appear to have been reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8b5693-a4a1-42e8-9e5f-7bf49fdb9744",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlp-shared-task",
   "language": "python",
   "name": "unlp-shared-task"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
